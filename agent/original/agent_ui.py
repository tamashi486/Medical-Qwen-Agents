import gradio as gr
import os
import sys
import time
import html
import base64
import io
import re
import uuid
import logging
from datetime import datetime
from typing import List, Tuple
from PIL import Image
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# --- Environment Setup ---
os.environ['no_proxy'] = 'localhost,127.0.0.1,0.0.0.0'
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,0.0.0.0'

# --- Configuration ---
MAX_MESSAGE_LENGTH = 16000
MAX_HISTORY_LENGTH = 50
MAX_FILE_SIZE = 10 * 1024 * 1024
MAX_OUTPUT_TOKENS = 8192
DEFAULT_OUTPUT_TOKENS = 2048

SUPPORTED_TEXT_EXTENSIONS = {'.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.xml', '.csv'}
SUPPORTED_IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}

# --- Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("WebUI")

# --- Import Agent Core ---
try:
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.append(current_dir)
    
    from agent_main import MedicalAgentSystem
    logger.info("âœ… Agent Class Imported")
except Exception as e:
    logger.error(f"âŒ Agent Class Import Failed: {e}")
    MedicalAgentSystem = None

# --- Initialize System ---
agent_system = None
if MedicalAgentSystem:
    try:
        # Configuration matching agent_main.py defaults or environment
        CONFIG = {
            "db_path": "/data/home/yihui/LLM/data/medical_embedding",
            "embedding_model_path": "/data/home/yihui/LLM/bge-m3",
            "vllm_api_base": "http://localhost:8000/v1",
            "model_name": "qwen-medical"
        }
        logger.info("ğŸš€ Initializing Medical Agent System...")
        agent_system = MedicalAgentSystem(**CONFIG)
        logger.info("âœ… Medical Agent System Initialized")
    except Exception as e:
        logger.error(f"âŒ Medical Agent System Initialization Failed: {e}")

# --- Helper Functions ---

def generate_session_id():
    """ä¸ºæ¯ä¸ª Tab ç”Ÿæˆå”¯ä¸€ä¼šè¯ ID"""
    return str(uuid.uuid4())

def process_uploaded_file(file_path: str) -> Tuple[str, str]:
    if not file_path or not os.path.exists(file_path):
        return "", "æ–‡ä»¶ä¸å­˜åœ¨"
    
    file_size = os.path.getsize(file_path)
    if file_size > MAX_FILE_SIZE:
        return "", f"æ–‡ä»¶è¿‡å¤§ ({file_size / 1024 / 1024:.1f}MB)"
    
    file_name = os.path.basename(file_path)
    file_ext = os.path.splitext(file_name)[1].lower()
    
    try:
        if file_ext in SUPPORTED_TEXT_EXTENSIONS:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            return content, f"ğŸ“„ æ–‡ä»¶: {file_name} ({file_size} bytes)"
        
        elif file_ext in SUPPORTED_IMAGE_EXTENSIONS:
            with Image.open(file_path) as img:
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                img.thumbnail((800, 800))
                buffer = io.BytesIO()
                img.save(buffer, format='JPEG', quality=85)
                img_base64 = base64.b64encode(buffer.getvalue()).decode()
            return f"data:image/jpeg;base64,{img_base64}", f"ğŸ–¼ï¸ å›¾ç‰‡: {file_name}"
        
        return "", f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}"
    except Exception as e:
        return "", f"æ–‡ä»¶å¤„ç†é”™è¯¯: {str(e)[:100]}"

def get_timestamp() -> str:
    return datetime.now().strftime("%H:%M:%S")

def format_message(content: str, role: str) -> str:
    timestamp = get_timestamp()
    role_name = "æ‚¨" if role == "user" else "AI"
    return f"**[{timestamp}] {role_name}:** {content}"

def clean_content(content: str) -> str:
    """Remove timestamp and role prefix for processing"""
    return re.sub(r'\*\*\[.*?\] .*?:\*\* ', '', content)

def format_thinking(text: str) -> str:
    """Format <think> tags into collapsible details"""
    # Handle complete think blocks (support multiple blocks)
    pattern = r"<think>(.*?)</think>"
    
    def replace_func(match):
        content = match.group(1).strip()
        return f'''<details class="thought-bubble">
<summary>ğŸ§  æ€è€ƒè¿‡ç¨‹ (ç‚¹å‡»å±•å¼€)</summary>
<div class="thought-content">{content}</div>
</details>'''
    
    formatted_text = re.sub(pattern, replace_func, text, flags=re.DOTALL)
    
    # Handle incomplete think block (streaming)
    if "<think>" in formatted_text and "</think>" not in formatted_text:
        parts = formatted_text.split("<think>", 1)
        pre_content = parts[0]
        think_content = parts[1]
        return f'''{pre_content}<details class="thought-bubble" open>
<summary>ğŸ§  æ­£åœ¨æ€è€ƒ...</summary>
<div class="thought-content">{think_content}</div>
</details>'''
        
    return formatted_text

def export_conversation(history: List[dict]) -> str:
    if not history:
        return "æš‚æ— å¯¹è¯è®°å½•"
    export_text = f"# å¯¹è¯è®°å½•å¯¼å‡º\nå¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    for msg in history:
        role = "ç”¨æˆ·" if msg["role"] == "user" else "AIåŠ©æ‰‹"
        content = clean_content(msg["content"])
        export_text += f"## {role}\n{content}\n\n"
    return export_text

def handle_export(history: List[dict]):
    if not history:
        return None, "âš ï¸ æš‚æ— å¯¹è¯è®°å½•å¯å¯¼å‡º"
    try:
        export_content = export_conversation(history)
        filename = f"conversation_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(export_content)
        return filename, f"âœ… å¯¹è¯å·²æˆåŠŸå¯¼å‡ºåˆ° {filename}"
    except Exception as e:
        return None, f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)[:50]}"

def copy_last_response(history: List[dict]):
    if not history:
        return "âš ï¸ æš‚æ— å¯¹è¯è®°å½•"
    for msg in reversed(history):
        if msg["role"] == "assistant":
            content = clean_content(msg["content"])
            return f"âœ… å·²å‡†å¤‡å¤åˆ¶: {content[:50]}..."
    return "âš ï¸ æœªæ‰¾åˆ°AIå›å¤"

def on_like(x: gr.LikeData, session_id: str):
    """æ”¶é›†ç”¨æˆ·ç‚¹èµ/ç‚¹è¸©æ•°æ®"""
    user_feedback = "Liked" if x.liked else "Disliked"
    logger.info(f"Feedback [{session_id}]: {user_feedback} | Message: {x.value[:50]}...")
    gr.Info(f"æ„Ÿè°¢åé¦ˆï¼å·²è®°å½• ({user_feedback})")

# --- Core Logic ---
async def chat_stream(
    message: str, 
    history: List[dict], 
    mode: str, 
    session_id: str,
    uploaded_file=None, 
    temperature=0.7, 
    max_tokens=DEFAULT_OUTPUT_TOKENS
):
    if not message.strip() and not uploaded_file:
        yield history
        return

    # Prepare Input
    file_content, file_info = "", ""
    if uploaded_file:
        file_content, file_info = process_uploaded_file(uploaded_file.name)
    
    display_message = message
    if file_info:
        display_message += f"\n\n[{file_info}]"
    
    agent_input = message
    if file_content:
        agent_input += f"\n\n[æ–‡ä»¶å†…å®¹]\n{file_content}"

    # Update History (User)
    new_history = history + [{"role": "user", "content": format_message(display_message, "user")}]
    
    # Placeholder for AI
    loading_msg = "â³ æ­£åœ¨æ€è€ƒå¹¶æŸ¥è¯¢çŸ¥è¯†åº“..." if mode == "Agentæ¨¡å¼" else "â³ æ­£åœ¨ç”Ÿæˆ..."
    new_history.append({"role": "assistant", "content": format_message(loading_msg, "assistant")})
    yield new_history

    try:
        if not agent_system:
            raise Exception("ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—")

        bot_response = ""
        
        if mode == "Agentæ¨¡å¼":
            # Agent Mode (Sync)
            # ä½¿ç”¨ session_id ä¿æŒå¤šè½®å¯¹è¯è®°å¿†
            # æ³¨æ„ï¼šå¦‚æœåç«¯ chat æ˜¯åŒæ­¥çš„ï¼Œè¿™é‡Œä¼šé˜»å¡ã€‚å»ºè®®åç«¯å®ç° achat
            # è¿™é‡Œæ¼”ç¤ºå‡è®¾åç«¯è¿”å›å®Œæ•´å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿæµå¼æ‰“å­—æœºæ•ˆæœè®©ç”¨æˆ·æ„Ÿè§‰å¿«
            # å¦‚æœ agent_system æœ‰ achat æ–¹æ³•ï¼Œæœ€å¥½ç”¨ await agent_system.achat(...)
            if hasattr(agent_system, 'achat'):
                response = await agent_system.achat(agent_input, session_id=session_id)
            else:
                response = agent_system.chat(agent_input, session_id=session_id)
            
            bot_response = response
            
            # Format and yield final result
            formatted_response = format_thinking(bot_response)
            new_history[-1]["content"] = format_message(formatted_response, "assistant")
            yield new_history
            
        else: # æ™®é€šé—®ç­”æ¨¡å¼
            # Ordinary Mode (Streaming)
            llm = agent_system.llm
            if not llm:
                raise Exception("LLM component not initialized")
            
            messages = [SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—AIåŠ©æ‰‹ã€‚è¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œæ— éœ€æä¾›å‚è€ƒæ¥æºã€‚")]
            # Reconstruct history for LLM
            for msg in history:
                content = clean_content(msg["content"])
                if msg["role"] == "user":
                    messages.append(HumanMessage(content=content))
                else:
                    messages.append(AIMessage(content=content))
            messages.append(HumanMessage(content=agent_input))
            
            # Stream with parameters
            # ä½¿ç”¨ bind åŠ¨æ€ç»‘å®šå‚æ•°
            runnable = llm.bind(temperature=temperature, max_tokens=max_tokens)
            
            # ä½¿ç”¨ astream è¿›è¡Œå¼‚æ­¥æµå¼è¾“å‡º
            async for chunk in runnable.astream(messages):
                content = chunk.content
                bot_response += content
                formatted_response = format_thinking(bot_response)
                new_history[-1]["content"] = format_message(formatted_response, "assistant")
                yield new_history

            # Final yield
            formatted_response = format_thinking(bot_response)
            new_history[-1]["content"] = format_message(formatted_response, "assistant")
            yield new_history

    except Exception as e:
        logger.error(f"Error: {e}")
        new_history[-1]["content"] = format_message(f"é”™è¯¯: {str(e)}", "assistant")
        yield new_history

# --- UI ---
custom_css = """
/* æ›´åŠ ç°ä»£åŒ–çš„é…è‰² */
body { font-family: 'Helvetica Neue', Arial, sans-serif; }
#chatbot { 
    height: 700px !important; 
    border: 1px solid #e0e0e0;
    border-radius: 12px;
    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
}
/* æ€è€ƒæ°”æ³¡æ ·å¼ */
.thought-bubble {
    background-color: #f0f4f8;
    border-left: 4px solid #4a90e2;
    padding: 10px;
    margin-bottom: 10px;
    border-radius: 4px;
    font-size: 0.9em;
    color: #555;
}
.thought-content {
    margin-top: 8px;
    white-space: pre-wrap;
}
.message-timestamp {
    font-size: 0.8em;
    color: #666;
}
/* å¼•ç”¨æºæ ·å¼ */
a { color: #4a90e2; text-decoration: none; }
a:hover { text-decoration: underline; }
"""

with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue"), css=custom_css, title="Huatuo-Pro Medical") as demo:
    
    # çŠ¶æ€ç®¡ç†ï¼šä¸ºæ¯ä¸ªç”¨æˆ·åˆ†é…ç‹¬ç«‹çš„ SessionID
    session_state = gr.State(generate_session_id)
    
    with gr.Row():
        # å·¦ä¾§è¾¹æ ï¼šæ§åˆ¶é¢æ¿
        with gr.Column(scale=1, min_width=300):
            gr.Markdown("### ğŸ¥ åŒ»ç–—å¤§æ¨¡å‹")
            gr.Markdown("Based on Qwen3-32B")
            
            with gr.Group():
                mode_radio = gr.Radio(
                    ["Agentæ¨¡å¼", "æ™®é€šé—®ç­”æ¨¡å¼"], 
                    label="æ¨ç†æ¨¡å¼", 
                    value="Agentæ¨¡å¼",
                    info="Agentæ¨¡å¼å…·å¤‡æŸ¥åº“å’Œå·¥å…·è°ƒç”¨èƒ½åŠ›"
                )
                
                with gr.Accordion("ğŸ›ï¸ å‚æ•°è®¾ç½®", open=False):
                    temperature_slider = gr.Slider(0.0, 1.0, value=0.1, label="æ¸©åº¦ (Temperature)", info="åŒ»ç–—åœºæ™¯å»ºè®®ä½è¿·ä»¥ä¿è¯ä¸¥è°¨")
                    max_tokens_slider = gr.Slider(100, MAX_OUTPUT_TOKENS, value=DEFAULT_OUTPUT_TOKENS, label="Max Tokens")
            
            file_upload = gr.File(label="ä¸Šä¼ æ–‡ä»¶")
            
            gr.Markdown("#### ğŸ’¡ æç¤º")
            gr.Markdown("- è¯¢é—®ç–¾ç—…æ—¶è¯·æè¿°æ¸…æ¥šç—‡çŠ¶\n- æ¶‰åŠè¯ç‰©æ—¶è¯·å’¨è¯¢ç¦å¿Œç—‡\n- æ¨¡å‹å›ç­”ä»…ä¾›å‚è€ƒï¼Œä¸ä½œä¸ºæœ€ç»ˆåŒ»ç–—è¯Šæ–­")
            
            with gr.Row():
                export_btn = gr.Button("ğŸ“¥ å¯¼å‡ºå¯¹è¯")
                copy_btn = gr.Button("ğŸ“‹ å¤åˆ¶æœ€åå›å¤")
                clean_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary")
            
            export_file = gr.File(label="å¯¼å‡ºçš„æ–‡ä»¶", visible=False)
            status_display = gr.Textbox(label="çŠ¶æ€", interactive=False, max_lines=1)

        # å³ä¾§ï¼šèŠå¤©ä¸»çª—å£
        with gr.Column(scale=4):
            chatbot = gr.Chatbot(
                label="è¯Šæ–­å¯¹è¯",
                type="messages", 
                avatar_images=(None, "https://img.alicdn.com/imgextra/i4/O1CN01c26iB51UyR3MKMFma_!!6000000002586-2-tps-124-124.png"),
                show_copy_button=True,
                elem_id="chatbot"
            )
            
            with gr.Row():
                msg_input = gr.Textbox(
                    placeholder="è¯·è¾“å…¥æ‚¨çš„åŒ»ç–—å’¨è¯¢é—®é¢˜ (ä¾‹å¦‚: é«˜è¡€å‹æ‚£è€…èƒ½åƒé¦™è•‰å—ï¼Ÿ)",
                    show_label=False,
                    scale=9,
                    container=False
                )
                submit_btn = gr.Button("å‘é€", variant="primary", scale=1)

    # --- äº‹ä»¶ç»‘å®š ---
    
    input_params = [msg_input, chatbot, mode_radio, session_state, file_upload, temperature_slider, max_tokens_slider]
    
    # æäº¤æ¶ˆæ¯
    msg_input.submit(
        fn=chat_stream, 
        inputs=input_params, 
        outputs=chatbot,
        show_progress="hidden"
    ).then(lambda: "", None, msg_input)

    submit_btn.click(
        fn=chat_stream, 
        inputs=input_params, 
        outputs=chatbot
    ).then(lambda: "", None, msg_input)

    # ç‚¹èµäº‹ä»¶
    chatbot.like(on_like, [session_state], None)

    # é¢å¤–åŠŸèƒ½æŒ‰é’®
    clean_btn.click(lambda: [], None, chatbot)
    export_btn.click(handle_export, [chatbot], [export_file, status_display])
    copy_btn.click(copy_last_response, [chatbot], status_display)

if __name__ == "__main__":
    print(f"æ­£åœ¨å¯åŠ¨ Web UI...")
    # ç”Ÿäº§ç¯å¢ƒé…ç½®
    demo.queue(max_size=20) # å¼€å¯é˜Ÿåˆ—ï¼Œæ”¯æŒå¤šç”¨æˆ·å¹¶å‘
    try:
        demo.launch(
            server_name="0.0.0.0", 
            server_port=7860, 
            share=False,
            favicon_path=None
        )
    except OSError:
        print("ç«¯å£ 7860 è¢«å ç”¨ï¼Œå°è¯•ä½¿ç”¨ 7861...")
        demo.launch(
            server_name="0.0.0.0", 
            server_port=7861, 
            share=False,
            favicon_path=None
        )

import os
import time
import logging
import asyncio
from typing import List, Dict, Any

import torch
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.tools import tool, StructuredTool
from langchain_classic.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage
# from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever 
from langchain_core.output_parsers import StrOutputParser

from agent.core.interfaces import AgentInterface

# é…ç½®æ—¥å¿— (é¢è¯•ç‚¹: å¯è§‚æµ‹æ€§)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MedicalAgentSystem(AgentInterface):
    """
    ä¼ä¸šçº§åŒ»ç–— Agent ç³»ç»Ÿå°è£…
    ç‰¹ç‚¹: å•ä¾‹æ¨¡å¼æ€æƒ³ã€æ”¯æŒ Rerankã€æ”¯æŒå¯¹è¯è®°å¿†ã€å¼‚æ­¥è°ƒç”¨
    """
    def __init__(self, 
                 db_path: str, 
                 embedding_model_path: str, 
                 vllm_api_base: str, 
                 model_name: str,
                 device: str = None):
        
        self.db_path = db_path
        self.embedding_model_path = embedding_model_path
        self.vllm_api_base = vllm_api_base
        self.model_name = model_name
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # å†…éƒ¨ç»„ä»¶çŠ¶æ€
        self.llm = None
        self.retriever = None
        self.agent_executor = None
        # ç”¨äºå­˜å‚¨ä¸åŒ SessionID çš„èŠå¤©è®°å½• (ç”Ÿäº§ç¯å¢ƒé€šå¸¸å­˜ Redis)
        self.chat_histories: Dict[str, ChatMessageHistory] = {}
        # å®ä½“è®°å¿†: å­˜å‚¨æ‚£è€…ç”»åƒ {session_id: "ç”»åƒæ–‡æœ¬"}
        self.entity_memories: Dict[str, str] = {}
        
        self._initialize_system()

    def _initialize_system(self):
        """åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶"""
        try:
            logger.info(f"ğŸš€ åˆå§‹åŒ–ç³»ç»Ÿ... è®¾å¤‡: {self.device}")
            start_time = time.time()

            # 1. Embedding & VectorDB
            embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model_path, 
                model_kwargs={'device': self.device}
            )
            
            # ä½¿ç”¨ Chroma
            vector_db = Chroma(persist_directory=self.db_path, embedding_function=embeddings)
            
            # é¢è¯•ç‚¹: è½¬æ¢ä¸º Retrieverï¼Œè·å– Top-10 æ­¤æ—¶ä¸ºäº†åé¢çš„ Rerank åšå‡†å¤‡
            # å¦‚æœæ²¡æœ‰ Rerank æ¨¡å‹ï¼Œè¿™é‡Œç›´æ¥ K=3 ä¹Ÿå¯ä»¥ï¼Œä½†é¢è¯•è¦è¯´"ä¸ºäº†å¬å›ç‡è®¾å¤§äº† K"
            self.retriever = vector_db.as_retriever(search_kwargs={"k": 10})

            # 2. LLM (vLLM)
            self.llm = ChatOpenAI(
                model=self.model_name,
                openai_api_key="EMPTY",
                openai_api_base=self.vllm_api_base,
                temperature=0.1, # åŒ»ç–—åœºæ™¯ä½ç†µ
                max_tokens=4096,
                streaming=True   # æ”¯æŒæµå¼è¾“å‡º
            )

            # 3. å·¥å…·é“¾æ³¨å†Œ
            tools = [self._create_search_tool(), self._create_bmi_tool()]

            # 4. Prompt è®¾è®¡ (é¢è¯•ç‚¹: Role, Constraints, Format)
            prompt = ChatPromptTemplate.from_messages([
                ("system", 
                 "ä½ æ˜¯ä¸€ä¸ªåä¸º'åé©¼'çš„ä¸“ä¸šåŒ»ç–—AIåŠ©æ‰‹ã€‚\n"
                 "ã€å½“å‰æ‚£è€…ç”»åƒã€‘\n{patient_profile}\n\n"
                 "æ ¸å¿ƒåŸåˆ™ï¼š\n"
                 "1. ã€å¾ªè¯åŒ»å­¦ã€‘å›ç­”å¿…é¡»ä¸¥æ ¼åŸºäºå·¥å…·æ£€ç´¢åˆ°çš„ã€è¯æ®ã€‘ã€‚å¦‚æœè¯æ®ä¸è¶³ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ã€‚\n"
                 "2. ã€å¼•ç”¨æ¥æºã€‘åœ¨å›ç­”ç»“å°¾ï¼Œå¿…é¡»åˆ—å‡ºå‚è€ƒçš„è¯æ®æ¥æºï¼ˆå¦‚ä¹¦ç±åç§°ï¼‰ã€‚\n"
                 "3. ã€å®‰å…¨åˆè§„ã€‘ä¸¥ç¦æä¾›å…·ä½“çš„å¤„æ–¹å»ºè®®ï¼ˆå¦‚'æ¯å¤©åƒ3æ¬¡'ï¼‰ï¼Œåªèƒ½æä¾›é€šç”¨çš„æ²»ç–—æ–¹æ¡ˆå‚è€ƒã€‚\n"
                 "4. ã€æ‹’ç»å›ç­”ã€‘å¯¹äºéåŒ»ç–—æˆ–è¿æ³•é—®é¢˜ï¼ˆå¦‚åˆ¶é€ æ¯’è¯ï¼‰ï¼Œè¯·ç›´æ¥æ‹’ç»ã€‚"),
                MessagesPlaceholder(variable_name="chat_history"), # è®°å¿†æ§½ä½
                ("user", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ])

            # 5. æ„å»º Agent
            agent = create_tool_calling_agent(self.llm, tools, prompt)
            
            # 6. åŒ…è£…è®°å¿†åŠŸèƒ½çš„æ‰§è¡Œå™¨
            raw_executor = AgentExecutor(
                agent=agent, 
                tools=tools, 
                verbose=True,
                return_intermediate_steps=True # è¿”å›ä¸­é—´æ­¥éª¤ä»¥ä¾¿è°ƒè¯•
            )
            
            # ä½¿ç”¨ RunnableWithMessageHistory ç®¡ç†å¤šè½®å¯¹è¯
            self.agent_executor = RunnableWithMessageHistory(
                raw_executor,
                self._get_session_history,
                input_messages_key="input",
                history_messages_key="chat_history",
            )

            logger.info(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f}s")

        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise

    def _get_session_history(self, session_id: str) -> ChatMessageHistory:
        """è·å–æˆ–åˆ›å»ºä¼šè¯å†å² (é¢è¯•ç‚¹: Session Management)"""
        if session_id not in self.chat_histories:
            self.chat_histories[session_id] = ChatMessageHistory()
        return self.chat_histories[session_id]

    def _update_entity_memory(self, session_id: str, user_input: str):
        """
        å®ä½“è®°å¿†æ›´æ–° (Entity Memory Update)
        ä½¿ç”¨ LLM æå–ç”¨æˆ·ç”»åƒå¹¶æ›´æ–°
        """
        current_profile = self.entity_memories.get(session_id, "æš‚æ— ")
        
        extraction_prompt = (
            f"å½“å‰æ‚£è€…ç”»åƒ: {current_profile}\n"
            f"ç”¨æˆ·æ–°è¾“å…¥: {user_input}\n"
            "è¯·åŸºäºæ–°è¾“å…¥æ›´æ–°æ‚£è€…ç”»åƒã€‚åŒ…å«: å¹´é¾„ã€æ€§åˆ«ã€æ—¢å¾€ç—…å²ã€è¿‡æ•æºã€å½“å‰ç—‡çŠ¶ã€‚\n"
            "å¦‚æœè¾“å…¥ä¸­æ²¡æœ‰æ–°ä¿¡æ¯ï¼Œè¯·ä¿æŒåŸç”»åƒä¸å˜ã€‚\n"
            "è¯·ç›´æ¥è¾“å‡ºæ›´æ–°åçš„ç”»åƒæ‘˜è¦ï¼Œä¸è¦åºŸè¯ã€‚"
        )
        
        try:
            # ä½¿ç”¨ invoke è°ƒç”¨ LLM è¿›è¡Œæå– (åŒæ­¥è°ƒç”¨ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®å¼‚æ­¥)
            # è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œç›´æ¥å¤ç”¨ self.llm
            # æ³¨æ„: è¿™é‡Œå¯èƒ½ä¼šå¢åŠ å»¶è¿Ÿ
            response = self.llm.invoke([HumanMessage(content=extraction_prompt)])
            new_profile = response.content
            self.entity_memories[session_id] = new_profile
            logger.info(f"ğŸ“ [Entity Memory] Updated for {session_id}: {new_profile[:50]}...")
        except Exception as e:
            logger.error(f"âŒ Entity extraction failed: {e}")

    def _manage_summary_memory(self, session_id: str):
        """
        åˆ†å±‚è®°å¿†ç®¡ç† (Tiered Memory Management)
        å¦‚æœå†å²è®°å½•è¶…è¿‡é˜ˆå€¼ï¼Œè¿›è¡Œæ‘˜è¦å‹ç¼©
        """
        history = self._get_session_history(session_id)
        messages = history.messages
        
        # é˜ˆå€¼è®¾å®š: ä¿ç•™æœ€è¿‘ 10 æ¡ (5è½®) åŸå§‹å¯¹è¯
        MAX_RAW_HISTORY = 10
        
        if len(messages) > MAX_RAW_HISTORY + 2: # +2 buffer
            # åˆ‡åˆ†: éœ€è¦æ‘˜è¦çš„éƒ¨åˆ† vs ä¿ç•™çš„éƒ¨åˆ†
            to_summarize = messages[:-MAX_RAW_HISTORY]
            to_keep = messages[-MAX_RAW_HISTORY:]
            
            # ç”Ÿæˆæ‘˜è¦
            summary_prompt = "è¯·ç®€è¦æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„å†å²é‡ç‚¹ï¼Œä¿ç•™å…³é”®åŒ»ç–—ä¿¡æ¯:\n"
            for msg in to_summarize:
                role = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AI"
                summary_prompt += f"{role}: {msg.content}\n"
            
            try:
                response = self.llm.invoke([HumanMessage(content=summary_prompt)])
                summary_text = response.content
                
                # é‡æ„å†å²: [SystemMessage(Summary)] + [Raw Messages]
                # æ³¨æ„: LangChain çš„ ChatMessageHistory æ˜¯ append-only çš„ï¼Œè¿™é‡Œæˆ‘ä»¬éœ€è¦ç›´æ¥ä¿®æ”¹å†…éƒ¨ list
                # è¿™æ˜¯ä¸€ä¸ª hackï¼Œæ ‡å‡†åšæ³•æ˜¯ç”¨ ConversationSummaryBufferMemoryï¼Œä½†ä¸ºäº†æ¼”ç¤ºåŸç†æ‰‹åŠ¨å®ç°
                new_messages = [SystemMessage(content=f"ã€å†å²å¯¹è¯æ‘˜è¦ã€‘: {summary_text}")] + to_keep
                history.messages = new_messages
                
                logger.info(f"ğŸ§  [Summary Memory] Compressed history for {session_id}")
            except Exception as e:
                logger.error(f"âŒ Summary generation failed: {e}")

    # --- å·¥å…·å®šä¹‰ (ä½¿ç”¨é—­åŒ…æˆ–å®ä¾‹æ–¹æ³•) ---

    def _create_search_tool(self):
        @tool("search_medical_knowledge")
        def search_tool(query: str):
            """
            ã€å¿…é¡»ä½¿ç”¨ã€‘å½“ç”¨æˆ·è¯¢é—®å…·ä½“çš„ç–¾ç—…ã€ç—‡çŠ¶ã€è¯å“ã€ç¦å¿Œç—‡æˆ–æ²»ç–—æŒ‡å—æ—¶ï¼Œå¿…é¡»è°ƒç”¨æ­¤å·¥å…·ã€‚
            """
            # 1. æŸ¥è¯¢æ”¹å†™ (Query Rewriting)
            rewrite_prompt = (
                f"è¯·å°†ç”¨æˆ·çš„æœç´¢æŸ¥è¯¢ '{query}' æ”¹å†™ä¸ºä¸€ä¸ªæ›´é€‚åˆæ£€ç´¢åŒ»å­¦çŸ¥è¯†åº“çš„ç‹¬ç«‹æŸ¥è¯¢è¯­å¥ã€‚\n"
                "å»é™¤å£è¯­åŒ–è¡¨è¾¾ï¼Œæå–æ ¸å¿ƒåŒ»å­¦å®ä½“ã€‚\n"
                "ç›´æ¥è¾“å‡ºæ”¹å†™åçš„æŸ¥è¯¢ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"
            )
            try:
                rewritten_query = self.llm.invoke([HumanMessage(content=rewrite_prompt)]).content.strip()
                logger.info(f"ğŸ”„ [Query Rewrite] '{query}' -> '{rewritten_query}'")
            except Exception as e:
                logger.warning(f"Query rewrite failed: {e}")
                rewritten_query = query

            logger.info(f"ğŸ” æ­£åœ¨æ£€ç´¢: {rewritten_query}")
            
            # 2. æ£€ç´¢ (Recall)
            docs = self.retriever.invoke(rewritten_query)
            if not docs:
                return "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
            # 3. (æ¨¡æ‹Ÿ) é‡æ’åº (Rerank) - é¢è¯•ç‚¹
            # åœ¨å®é™…å¤§å‚ä»£ç ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ BGE-Reranker æ¨¡å‹å¯¹ docs æ‰“åˆ†
            # sorted_docs = reranker.rank(query, docs)[:3] 
            # è¿™é‡Œä¸ºäº†ä»£ç å¯è¿è¡Œï¼Œæˆ‘ä»¬ç®€å•æˆªå–å‰ 3 ä¸ª
            final_docs = docs[:3]

            # 4. æ ¼å¼åŒ–è¾“å‡º (å¸¦å…ƒæ•°æ®) - é¢è¯•ç‚¹
            results = []
            for i, doc in enumerate(final_docs):
                source = doc.metadata.get('title', 'æœªçŸ¥æ¥æº')
                category = doc.metadata.get('category', 'é€šç”¨')
                content = doc.page_content.replace('\n', ' ')
                results.append(f"[è¯æ®{i+1}] (æ¥æº: {source} | åˆ†ç±»: {category}):\n{content}")
            
            return "\n\n".join(results)
        return search_tool

    def _create_bmi_tool(self):
        @tool("calculate_bmi")
        def bmi_tool(weight_kg: float, height_m: float):
            """è®¡ç®—ç”¨æˆ·çš„BMIæŒ‡æ•°ã€‚è¾“å…¥ä½“é‡(kg)å’Œèº«é«˜(m)ã€‚"""
            try:
                bmi = weight_kg / (height_m ** 2)
                status = "æ­£å¸¸"
                if bmi < 18.5: status = "åç˜¦"
                elif bmi > 24: status = "è¶…é‡"
                
                return f"BMIæ•°å€¼: {bmi:.2f}\nå¥åº·çŠ¶æ€: {status}\nå»ºè®®: è¯·ç»“åˆå…·ä½“èº«ä½“çŠ¶å†µå’¨è¯¢åŒ»ç”Ÿã€‚"
            except Exception as e:
                return f"è®¡ç®—å‡ºé”™: {str(e)}"
        return bmi_tool

    def _route_request(self, user_input: str) -> str:
        """
        è·¯ç”±æ¨¡å¼ (Router Pattern)
        åˆ¤æ–­ç”¨æˆ·æ„å›¾: chat, medical, complex
        """
        router_prompt = (
            f"ç”¨æˆ·è¾“å…¥: {user_input}\n"
            "è¯·åˆ¤æ–­ç”¨æˆ·æ„å›¾ï¼Œè¿”å›ä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€:\n"
            "- chat: é—²èŠã€é—®å€™ã€éåŒ»ç–—é—®é¢˜\n"
            "- medical: å…·ä½“çš„åŒ»ç–—å’¨è¯¢ã€æŸ¥ç—…ã€æŸ¥è¯\n"
            "- complex: å¤æ‚çš„ç—…ä¾‹åˆ†æã€å¤šæ­¥æ¨ç†\n"
            "ç›´æ¥è¾“å‡ºç±»åˆ«åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"
        )
        try:
            intent = self.llm.invoke([HumanMessage(content=router_prompt)]).content.strip().lower()
            # ç®€å•æ¸…æ´—
            if "medical" in intent: return "medical"
            if "complex" in intent: return "complex"
            return "chat"
        except Exception:
            return "medical" # é»˜è®¤èµ°åŒ»ç–—

    def _reflection_check(self, user_input: str, response: str) -> str:
        """
        åæ€æ¨¡å¼ (Reflection Pattern)
        æ£€æŸ¥å›ç­”æ˜¯å¦åŒ…å«å¹»è§‰æˆ–è¿è§„
        """
        critique_prompt = (
            f"ç”¨æˆ·é—®é¢˜: {user_input}\n"
            f"AIå›ç­”: {response}\n"
            "è¯·ä½œä¸º'åŒ»ç–—å®¡æ ¸å‘˜'æ£€æŸ¥ä¸Šè¿°å›ç­”ï¼š\n"
            "1. æ˜¯å¦åŒ…å«å…·ä½“çš„å¤„æ–¹å»ºè®®ï¼ˆå¦‚'æ¯å¤©åƒ3æ¬¡'ï¼‰ï¼Ÿ(è¿è§„)\n"
            "2. æ˜¯å¦å¼•ç”¨äº†ä¸å­˜åœ¨çš„è¯æ®ï¼Ÿ(å¹»è§‰)\n"
            "3. æ˜¯å¦å›ç­”äº†éåŒ»ç–—é—®é¢˜ä½†ä¼ªè£…æˆåŒ»ç–—å»ºè®®ï¼Ÿ\n"
            "å¦‚æœå›ç­”å®‰å…¨ä¸”åˆè§„ï¼Œè¯·è¾“å‡º 'PASS'ã€‚\n"
            "å¦‚æœæœ‰é—®é¢˜ï¼Œè¯·è¾“å‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®ã€‚"
        )
        try:
            critique = self.llm.invoke([HumanMessage(content=critique_prompt)]).content.strip()
            if "PASS" in critique:
                return response
            
            logger.warning(f"âš ï¸ [Reflection] Critique triggered: {critique}")
            # Regeneration
            fix_prompt = (
                f"åŸé—®é¢˜: {user_input}\n"
                f"åŸå›ç­”: {response}\n"
                f"å®¡æ ¸æ„è§: {critique}\n"
                "è¯·æ ¹æ®å®¡æ ¸æ„è§é‡å†™å›ç­”ï¼Œç¡®ä¿å®‰å…¨åˆè§„ã€‚"
            )
            new_response = self.llm.invoke([HumanMessage(content=fix_prompt)]).content
            return new_response
        except Exception as e:
            logger.error(f"Reflection failed: {e}")
            return response

    async def _route_request_async(self, user_input: str) -> str:
        """
        è·¯ç”±æ¨¡å¼ (Router Pattern) - å¼‚æ­¥ç‰ˆ
        """
        router_prompt = (
            f"ç”¨æˆ·è¾“å…¥: {user_input}\n"
            "è¯·åˆ¤æ–­ç”¨æˆ·æ„å›¾ï¼Œè¿”å›ä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€:\n"
            "- chat: é—²èŠã€é—®å€™ã€éåŒ»ç–—é—®é¢˜\n"
            "- medical: å…·ä½“çš„åŒ»ç–—å’¨è¯¢ã€æŸ¥ç—…ã€æŸ¥è¯\n"
            "- complex: å¤æ‚çš„ç—…ä¾‹åˆ†æã€å¤šæ­¥æ¨ç†\n"
            "ç›´æ¥è¾“å‡ºç±»åˆ«åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"
        )
        try:
            response = await self.llm.ainvoke([HumanMessage(content=router_prompt)])
            intent = response.content.strip().lower()
            if "medical" in intent: return "medical"
            if "complex" in intent: return "complex"
            return "chat"
        except Exception:
            return "medical"

    async def _reflection_check_async(self, user_input: str, response: str) -> str:
        """
        åæ€æ¨¡å¼ (Reflection Pattern) - å¼‚æ­¥ç‰ˆ
        """
        critique_prompt = (
            f"ç”¨æˆ·é—®é¢˜: {user_input}\n"
            f"AIå›ç­”: {response}\n"
            "è¯·ä½œä¸º'åŒ»ç–—å®¡æ ¸å‘˜'æ£€æŸ¥ä¸Šè¿°å›ç­”ï¼š\n"
            "1. æ˜¯å¦åŒ…å«å…·ä½“çš„å¤„æ–¹å»ºè®®ï¼ˆå¦‚'æ¯å¤©åƒ3æ¬¡'ï¼‰ï¼Ÿ(è¿è§„)\n"
            "2. æ˜¯å¦å¼•ç”¨äº†ä¸å­˜åœ¨çš„è¯æ®ï¼Ÿ(å¹»è§‰)\n"
            "3. æ˜¯å¦å›ç­”äº†éåŒ»ç–—é—®é¢˜ä½†ä¼ªè£…æˆåŒ»ç–—å»ºè®®ï¼Ÿ\n"
            "å¦‚æœå›ç­”å®‰å…¨ä¸”åˆè§„ï¼Œè¯·è¾“å‡º 'PASS'ã€‚\n"
            "å¦‚æœæœ‰é—®é¢˜ï¼Œè¯·è¾“å‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®ã€‚"
        )
        try:
            critique_res = await self.llm.ainvoke([HumanMessage(content=critique_prompt)])
            critique = critique_res.content.strip()
            if "PASS" in critique:
                return response
            
            logger.warning(f"âš ï¸ [Reflection] Critique triggered: {critique}")
            # Regeneration
            fix_prompt = (
                f"åŸé—®é¢˜: {user_input}\n"
                f"åŸå›ç­”: {response}\n"
                f"å®¡æ ¸æ„è§: {critique}\n"
                "è¯·æ ¹æ®å®¡æ ¸æ„è§é‡å†™å›ç­”ï¼Œç¡®ä¿å®‰å…¨åˆè§„ã€‚"
            )
            new_response_res = await self.llm.ainvoke([HumanMessage(content=fix_prompt)])
            return new_response_res.content
        except Exception as e:
            logger.error(f"Reflection failed: {e}")
            return response

    # --- å¯¹å¤–æ¥å£ ---

    def chat(self, user_input: str, session_id: str = "default_user"):
        """åŒæ­¥è°ƒç”¨æ¥å£"""
        try:
            # 0. è·¯ç”±
            intent = self._route_request(user_input)
            logger.info(f"ğŸš¦ [Router] Intent: {intent}")
            
            if intent == "chat":
                return self.llm.invoke([HumanMessage(content=user_input)]).content

            # 1. æ›´æ–°å®ä½“è®°å¿†
            self._update_entity_memory(session_id, user_input)
            
            # 2. ç®¡ç†æ‘˜è¦è®°å¿†
            self._manage_summary_memory(session_id)
            
            # 3. è·å–å½“å‰ç”»åƒ
            patient_profile = self.entity_memories.get(session_id, "æœªçŸ¥")

            # 4. Agent æ‰§è¡Œ
            response = self.agent_executor.invoke(
                {"input": user_input, "patient_profile": patient_profile},
                config={"configurable": {"session_id": session_id}}
            )
            final_output = response["output"]

            # 5. åæ€æ£€æŸ¥
            final_output = self._reflection_check(user_input, final_output)
            
            return final_output
        except Exception as e:
            logger.error(f"æ¨ç†é”™è¯¯: {e}")
            return "ç³»ç»Ÿæ­£å¦‚ç«å¦‚è¼åœ°ç»´ä¿®ä¸­..."

    async def achat(self, user_input: str, session_id: str = "default_user", mode: str = "agent"):
        """å¼‚æ­¥è°ƒç”¨æ¥å£ (WebæœåŠ¡ä¸“ç”¨)"""
        try:
            # 0. è·¯ç”± (ä»…åœ¨ Agent æ¨¡å¼ä¸‹ç”Ÿæ•ˆ)
            if mode == "agent":
                intent = await self._route_request_async(user_input)
                logger.info(f"ğŸš¦ [Router] Intent: {intent}")
                if intent == "chat":
                    response = await self.llm.ainvoke([HumanMessage(content=user_input)])
                    return response.content

            # 1. æ›´æ–°å®ä½“è®°å¿† (åå°ä»»åŠ¡ï¼Œä¸é˜»å¡ä¸»æµç¨‹)
            # ä½¿ç”¨ asyncio.create_task å°†å…¶æ”¾å…¥åå°æ‰§è¡Œ
            asyncio.create_task(asyncio.to_thread(self._update_entity_memory, session_id, user_input))
            
            # 2. ç®¡ç†æ‘˜è¦è®°å¿† (åå°ä»»åŠ¡)
            asyncio.create_task(asyncio.to_thread(self._manage_summary_memory, session_id))
            
            # 3. è·å–å½“å‰ç”»åƒ (ç›´æ¥è¯»å–ï¼Œä¸ç­‰å¾…æ›´æ–°)
            patient_profile = self.entity_memories.get(session_id, "æœªçŸ¥")

            if mode == "agent":
                response = await self.agent_executor.ainvoke(
                    {"input": user_input, "patient_profile": patient_profile},
                    config={"configurable": {"session_id": session_id}}
                )
                final_output = response["output"]
                
                # 5. åæ€æ£€æŸ¥ (å¼‚æ­¥)
                final_output = await self._reflection_check_async(user_input, final_output)
                return final_output
            else:
                # Simple Chat Mode
                history = self._get_session_history(session_id)
                messages = [SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—AIåŠ©æ‰‹ã€‚è¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œæ— éœ€æä¾›å‚è€ƒæ¥æºã€‚")]
                messages.extend(history.messages)
                messages.append(HumanMessage(content=user_input))
                
                # Manually update history
                history.add_user_message(user_input)
                
                response_content = ""
                # Note: This is not streaming to the client in real-time via API yet, 
                # but returns the full response asynchronously.
                async for chunk in self.llm.astream(messages):
                    response_content += chunk.content
                
                history.add_ai_message(response_content)
                return response_content

        except Exception as e:
            logger.error(f"å¼‚æ­¥æ¨ç†é”™è¯¯: {e}")
            return "ç³»ç»Ÿç¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚"

    def get_history(self, session_id: str) -> List[Dict[str, str]]:
        """è·å–ä¼šè¯å†å²"""
        if session_id not in self.chat_histories:
            return []
        
        history = self.chat_histories[session_id]
        formatted_history = []
        for msg in history.messages:
            if isinstance(msg, HumanMessage):
                formatted_history.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                formatted_history.append({"role": "assistant", "content": msg.content})
        return formatted_history

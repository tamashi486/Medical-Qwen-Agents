import json
import os
import random
import re
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================
# 1. æ¨¡å‹è·¯å¾„
MODEL_PATH = "../../qwen3-32B" 

# 2. è¾“å…¥æ•°æ® (Alpaca æ ¼å¼ jsonl)
INPUT_FILE = "data/train.jsonl"

# 3. è¾“å‡ºæ–‡ä»¶ (DPO æ ¼å¼ JSONL)
OUTPUT_FILE = "data/dpo.jsonl"

# 4. é‡‡æ ·é…ç½®
TARGET_SAMPLE_SIZE = 50000  # ç›®æ ‡é‡‡æ ·æ•°é‡
RANDOM_SEED = 42            # å›ºå®šç§å­ï¼Œä¿è¯æ¯æ¬¡é‡‡æ ·çš„5ä¸‡æ¡æ•°æ®æ˜¯ä¸€æ ·çš„

# 5. ç¡¬ä»¶é…ç½®
TENSOR_PARALLEL_SIZE = 4    # 4å¡å¹¶è¡Œ

def generate_dpo_dataset():
    print(f"ğŸš€ ä»»åŠ¡å¯åŠ¨ï¼šç”Ÿæˆ DPO æ•°æ® (ç›®æ ‡: {TARGET_SAMPLE_SIZE} æ¡)")
    
    # --- 1. è¯»å–æ•°æ® ---
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
        return

    print(f"ğŸ“– è¯»å–å…¨é‡è¾“å…¥æ•°æ®: {INPUT_FILE} ...")
    data_list = []
    
    # åˆ¤æ–­æ–‡ä»¶æ ¼å¼æ˜¯ JSON è¿˜æ˜¯ JSONL
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    data_list.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    
    total_original = len(data_list)
    print(f"ğŸ“Š åŸå§‹æ•°æ®æ€»é‡: {total_original}")

    # --- 2. éšæœºé‡‡æ · (å›ºå®šç§å­) ---
    if total_original > TARGET_SAMPLE_SIZE:
        print(f"âœ‚ï¸ æ­£åœ¨è¿›è¡Œéšæœºé‡‡æ · (Seed={RANDOM_SEED})...")
        random.seed(RANDOM_SEED)
        target_data = random.sample(data_list, TARGET_SAMPLE_SIZE)
    else:
        print(f"âš ï¸ æ•°æ®ä¸è¶³ {TARGET_SAMPLE_SIZE}ï¼Œä½¿ç”¨å…¨é‡æ•°æ®ã€‚")
        target_data = data_list

    # --- 3. æ„å»º Prompts ---
    print("ğŸ”„ æ­£åœ¨æ„å»º Promptsï¼ŒåŠ è½½ Tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    except Exception as e:
        print(f"âŒ Tokenizer åŠ è½½å¤±è´¥: {e}")
        return

    prompts = []
    # è¿™é‡Œçš„ messages ç”¨äºæ„å»º promptï¼Œtarget_data ç”¨äºåç»­ç»„è£… jsonl
    for entry in target_data:
        instruction = entry.get("instruction", "")
        input_text = entry.get("input", "")
        
        # System Prompt: å¼ºåˆ¶ç›´æ¥å›ç­”ï¼ŒæŠ‘åˆ¶æ€è€ƒæ¨¡å¼
        messages = [
            {"role": "system", "content": "You are a helpful assistant. Answer directly and concisely."},
            {"role": "user", "content": instruction + input_text}
        ]
        
        # å°è¯•ç¦ç”¨ thinking æ¨¡å¼
        try:
            text_prompt = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True,
                enable_thinking=False 
            )
        except TypeError:
            # å…¼å®¹æ—§ç‰ˆæœ¬
            text_prompt = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
        prompts.append(text_prompt)

    # --- 4. åˆå§‹åŒ– vLLM å¹¶ç”Ÿæˆ ---
    print(f"ğŸš€ åˆå§‹åŒ– vLLM å¼•æ“ (TP={TENSOR_PARALLEL_SIZE})...")
    try:
        llm = LLM(
            model=MODEL_PATH,
            tensor_parallel_size=TENSOR_PARALLEL_SIZE,
            trust_remote_code=True,
            dtype="bfloat16",
            gpu_memory_utilization=0.90, # æ¦¨å¹²æ˜¾å­˜ï¼Œæœ€å¤§åŒ–åå
            enforce_eager=False,
        )
    except Exception as e:
        print(f"âŒ vLLM å¼•æ“åŠ è½½å¤±è´¥: {e}")
        return

    print(f"âš¡ å¼€å§‹æé€Ÿç”Ÿæˆ {len(prompts)} æ¡æ•°æ® (ç”± vLLM è‡ªåŠ¨è°ƒåº¦)...")
    
    # é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.8,
        top_p=0.95,
        max_tokens=512,
        stop=["<|endoftext|>", "<|im_end|>"]
    )

    # ç›´æ¥ä¼ å…¥æ‰€æœ‰ promptï¼ŒvLLM æ•ˆç‡æœ€é«˜
    outputs = llm.generate(prompts, sampling_params)

    # --- 5. æ¸…æ´—ä¸å†™å…¥ ---
    print(f"ğŸ’¾ æ­£åœ¨å†™å…¥æ–‡ä»¶: {OUTPUT_FILE} ...")
    
    # å‡†å¤‡æ­£åˆ™æ¸…æ´— (é˜²æ­¢ Thinking æ¨¡å¼æ³„æ¼)
    think_pattern = re.compile(r"<think>.*?</think>", re.DOTALL)
    
    # ä½¿ç”¨ 'w' æ¨¡å¼è¦†ç›–å†™å…¥ï¼Œä¸éœ€è¦æ–­ç‚¹ç»­ä¼ 
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        # outputs çš„é¡ºåºä¸¥æ ¼å¯¹åº” prompts/target_data çš„é¡ºåº
        for i, output in enumerate(outputs):
            generated_text = output.outputs[0].text.strip()
            
            # æ­£åˆ™æ¸…æ´—ï¼šå¼ºåˆ¶åˆ é™¤ <think> æ ‡ç­¾åŠå…¶å†…å®¹
            if "<think>" in generated_text:
                generated_text = re.sub(think_pattern, "", generated_text).strip()
            
            # å¤„ç†ç©ºå›å¤
            if not generated_text:
                generated_text = "No response."

            original = target_data[i]
            
            dpo_entry = {
                "instruction": original["instruction"],
                "input": original["input"],
                "chosen": original["output"], # æ­£æ ·æœ¬ (Huatuo)
                "rejected": generated_text    # è´Ÿæ ·æœ¬ (Qwen3 Generated)
            }
            
            # ç®€å•å»é‡ï¼šå¦‚æœç”Ÿæˆçš„å’Œæ­£ç¡®ç­”æ¡ˆå®Œå…¨ä¸€æ ·ï¼Œå¯¹ DPO è´¡çŒ®ä¸º 0ï¼Œå¯é€‰æ‹©è·³è¿‡
            # è¿™é‡Œä¸ºäº†ä¿æŒæ•°æ®å®Œæ•´æ€§ï¼Œä¿ç•™å†™å…¥
            if dpo_entry["chosen"] == dpo_entry["rejected"]:
                # print("Skip identical sample") 
                continue

            f.write(json.dumps(dpo_entry, ensure_ascii=False) + "\n")

    print("="*40)
    print("âœ… DPO æ•°æ®ç”Ÿæˆå®Œæ¯•ï¼")
    print(f"ğŸ“Š æ–‡ä»¶è·¯å¾„: {OUTPUT_FILE}")
    print("="*40)

if __name__ == "__main__":
    generate_dpo_dataset()